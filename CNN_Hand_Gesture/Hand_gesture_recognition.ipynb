{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Gesture Recognition\n",
    "\n",
    "## American Sign Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "kZjsHTSWOfbc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Grnhsu-GOr5y"
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"./dataset/sign_mnist_train.csv\")\n",
    "data_test = pd.read_csv(\"./dataset/sign_mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      3     107     118     127     134     139     143     146     150   \n",
       "1      6     155     157     156     156     156     157     156     158   \n",
       "2      2     187     188     188     187     187     186     187     188   \n",
       "3      2     211     211     212     212     211     210     211     210   \n",
       "4     13     164     167     170     172     176     179     180     184   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     153  ...       207       207       207       207       206       206   \n",
       "1     158  ...        69       149       128        87        94       163   \n",
       "2     187  ...       202       201       200       199       198       199   \n",
       "3     210  ...       235       234       233       231       230       226   \n",
       "4     185  ...        92       105       105       108       133       163   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       206       204       203       202  \n",
       "1       175       103       135       149  \n",
       "2       198       195       194       195  \n",
       "3       225       222       229       163  \n",
       "4       157       163       164       179  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img_label, dataframe):\n",
    "    \n",
    "    label = dataframe['label'][img_label]\n",
    "    pixels = dataframe.iloc[img_label, 1:]\n",
    "    # turn into unsigned 8 bits\n",
    "    pixels = np.array(pixels, dtype='uint8')\n",
    "    # convert to 2d array\n",
    "    pixels = pixels.reshape((28, 28))\n",
    "    \n",
    "    plt.title(f\"Label is {label}\")\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(20, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting labels\n",
    "\n",
    "y_train = data_train['label']\n",
    "y_test = data_test['label']\n",
    "\n",
    "# remove the labels\n",
    "del data_train['label']\n",
    "del data_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Binarizer\n",
    "# Convert categorical values to 1s and 0s\n",
    "\n",
    "label_binarizer = LabelBinarizer()\n",
    "\n",
    "y_train = label_binarizer.fit_transform(y_train)\n",
    "y_test = label_binarizer.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_train.values\n",
    "x_test = data_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing pixels to between 0 and 1\n",
    "\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping\n",
    "\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data  Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation to improves model to reduce overfitting\n",
    "\n",
    "data_generator = ImageDataGenerator(\n",
    "                    # if True, input mean will be 0\n",
    "                    featurewise_center = False,\n",
    "                    # if True, sample mean will be 0\n",
    "                    samplewise_center = False,\n",
    "                    # If True, input will be standard of the data set \n",
    "                    featurewise_std_normalization = False,\n",
    "                    samplewise_std_normalization = False,\n",
    "                    # Reduces redundancy of matrix of pixel images (when background is not clear)\n",
    "                    zca_whitening = False,\n",
    "                    # Rotates imageat random to set value \n",
    "                    rotation_range = 10,\n",
    "                    # Zoom into image at random\n",
    "                    zoom_range = 0.1,\n",
    "                    # Shift image horizontally at random depedning on the range\n",
    "                    width_shift_range = 0.1,\n",
    "                    # Shift image vertically at random depedning on the range\n",
    "                    height_shift_range = 0.1,\n",
    "                    # Flips images horizontally at random\n",
    "                    horizontal_flip = False,\n",
    "                    # Flips images vertically at random\n",
    "                    vertical_flip = False\n",
    "                )\n",
    "\n",
    "\n",
    "data_generator.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters = 75, kernel_size = (3,3),\n",
    "        strides = 1, padding = 'same',\n",
    "        activation = 'relu',\n",
    "        #input shape only for first layer\n",
    "        input_shape = (28, 28, 1)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "model.add(\n",
    "    # Helps to process data faster and stable by re-centering data \n",
    "    BatchNormalization()\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    MaxPool2D(\n",
    "        pool_size = (2,2), strides = 2,\n",
    "        padding = 'same'\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters = 50, kernel_size = (3,3),\n",
    "        strides = 1, padding = 'same',\n",
    "        activation = 'relu',\n",
    "        input_shape = (28, 28, 1)\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    # Randomly drop 20% to prevent overfitting\n",
    "    Dropout(0.2)\n",
    ")\n",
    "\n",
    "\n",
    "model.add(\n",
    "    # Helps to process data faster and stable by re-centering data \n",
    "    BatchNormalization()\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    MaxPool2D(\n",
    "        pool_size = (2,2), strides = 2,\n",
    "        padding = 'same'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters = 25, kernel_size = (3,3),\n",
    "        strides = 1, padding = 'same',\n",
    "        activation = 'relu',\n",
    "        input_shape = (28, 28, 1)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "model.add(\n",
    "    # Helps to process data faster and stable by re-centering data \n",
    "    BatchNormalization()\n",
    ")\n",
    "\n",
    "\n",
    "model.add(\n",
    "    MaxPool2D(\n",
    "        pool_size = (2,2), strides = 2,\n",
    "        padding = 'same'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "model.add(\n",
    "    # This layer reshapes tensor so shape is equal to number of elements in tensor\n",
    "    Flatten()\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    # Fully connected layer\n",
    "    Dense(\n",
    "        units = 512, activation = 'relu'\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    # Randomly drop 30% to prevent overfitting\n",
    "    Dropout(0.3)\n",
    ")\n",
    "\n",
    "\n",
    "model.add(\n",
    "    # Fully connected layer\n",
    "    Dense(\n",
    "        units = 24, activation = 'softmax'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduces learning rate when metric has stops improving\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(\n",
    "                            monitor = 'val_accuracy', patience = 2,\n",
    "                            verbose = 1, factor = 0.5,\n",
    "                            min_lr = 0.00001\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "215/215 [==============================] - 80s 367ms/step - loss: 0.2917 - accuracy: 0.8990 - val_loss: 1.7663 - val_accuracy: 0.4756\n",
      "Epoch 2/20\n",
      "215/215 [==============================] - 76s 355ms/step - loss: 0.1064 - accuracy: 0.9667 - val_loss: 1.1413 - val_accuracy: 0.6665\n",
      "Epoch 3/20\n",
      "215/215 [==============================] - 84s 390ms/step - loss: 0.0646 - accuracy: 0.9783 - val_loss: 0.0489 - val_accuracy: 0.9828\n",
      "Epoch 4/20\n",
      "215/215 [==============================] - 82s 382ms/step - loss: 0.0411 - accuracy: 0.9870 - val_loss: 0.0335 - val_accuracy: 0.9895\n",
      "Epoch 5/20\n",
      "215/215 [==============================] - 85s 395ms/step - loss: 0.0390 - accuracy: 0.9875 - val_loss: 0.0920 - val_accuracy: 0.9700\n",
      "Epoch 6/20\n",
      "215/215 [==============================] - 86s 398ms/step - loss: 0.0254 - accuracy: 0.9923 - val_loss: 0.0588 - val_accuracy: 0.9784\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 7/20\n",
      "215/215 [==============================] - 82s 379ms/step - loss: 0.0200 - accuracy: 0.9931 - val_loss: 0.0094 - val_accuracy: 0.9965\n",
      "Epoch 8/20\n",
      "215/215 [==============================] - 88s 411ms/step - loss: 0.0125 - accuracy: 0.9963 - val_loss: 0.1748 - val_accuracy: 0.9308\n",
      "Epoch 9/20\n",
      "215/215 [==============================] - 88s 411ms/step - loss: 0.0132 - accuracy: 0.9955 - val_loss: 0.0102 - val_accuracy: 0.9968\n",
      "Epoch 10/20\n",
      "215/215 [==============================] - 83s 384ms/step - loss: 0.0110 - accuracy: 0.9962 - val_loss: 0.0184 - val_accuracy: 0.9916\n",
      "Epoch 11/20\n",
      "215/215 [==============================] - 78s 362ms/step - loss: 0.0091 - accuracy: 0.9969 - val_loss: 0.0134 - val_accuracy: 0.9934\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 12/20\n",
      "215/215 [==============================] - 79s 368ms/step - loss: 0.0081 - accuracy: 0.9974 - val_loss: 0.0114 - val_accuracy: 0.9972\n",
      "Epoch 13/20\n",
      "215/215 [==============================] - 79s 369ms/step - loss: 0.0077 - accuracy: 0.9975 - val_loss: 0.0026 - val_accuracy: 0.9990\n",
      "Epoch 14/20\n",
      "215/215 [==============================] - 78s 364ms/step - loss: 0.0054 - accuracy: 0.9983 - val_loss: 0.0129 - val_accuracy: 0.9968\n",
      "Epoch 15/20\n",
      "215/215 [==============================] - 82s 383ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.0067 - val_accuracy: 0.9979\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 16/20\n",
      "215/215 [==============================] - 83s 385ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 0.9997\n",
      "Epoch 17/20\n",
      "215/215 [==============================] - 83s 388ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.0036 - val_accuracy: 0.9986\n",
      "Epoch 18/20\n",
      "215/215 [==============================] - 81s 374ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0029 - val_accuracy: 0.9986\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 19/20\n",
      "215/215 [==============================] - 78s 361ms/step - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.0034 - val_accuracy: 0.9982\n",
      "Epoch 20/20\n",
      "215/215 [==============================] - 82s 382ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0023 - val_accuracy: 0.9990\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23a45a4cee0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    data_generator.flow(x_train, y_train, batch_size = 128),\n",
    "    epochs = 20, validation_data = (x_test, y_test),\n",
    "    callbacks = [learning_rate_reduction]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 4s 17ms/step - loss: 0.0023 - accuracy: 0.9990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.002317276317626238, 0.999023973941803]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  5, 10,  0,  3], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(x_test)\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    if(predictions[i] >= 9 or predictions[i] >=25):\n",
    "        predictions[i] += 1\n",
    "    \n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('hand_gesture.h5')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hand_gesture_recognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
